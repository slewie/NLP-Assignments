{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "from collections import Counter\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\", device_map='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpellChecker:\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "    \n",
        "    def spell_check(self, sentence: str):\n",
        "        \"\"\"Spell check a sentence using a dictionary of words.\n",
        "        Parameters:\n",
        "            - sentence (str): The sentence to be spell checked.\n",
        "        Returns:\n",
        "            - str: The corrected sentence with any misspelled words replaced.\n",
        "        Processing Logic:\n",
        "            - Split the sentence into a list of words.\n",
        "            - Check each word in the list for spelling errors.\n",
        "            - If a word is misspelled, replace it with the most likely correct word.\n",
        "            - Join the corrected words back into a sentence.\"\"\"\n",
        "        sentence_list = sentence.split()\n",
        "        res = [self._spell_check_at(sentence_list, i) if sentence_list[i].lower() not in WORDS else sentence_list[i] for i in range(len(sentence_list))]\n",
        "    \n",
        "        corrected_sentence = []\n",
        "        for my_dict in res:\n",
        "            if isinstance(my_dict, str):\n",
        "                corrected_sentence.append(my_dict)\n",
        "                continue\n",
        "            top_keys = sorted(my_dict, key=my_dict.get, reverse=True)[:1][0]\n",
        "            corrected_sentence.append(top_keys)\n",
        "            \n",
        "        return \" \".join(corrected_sentence)\n",
        "    \n",
        "    def _levenshtein_distance(self, s1: str, s2: str):\n",
        "        \"\"\"Calculates the Levenshtein distance between two strings.\n",
        "        Parameters:\n",
        "            - s1 (str): First string.\n",
        "            - s2 (str): Second string.\n",
        "        Returns:\n",
        "            - float: Normalized Levenshtein distance between the two strings.\n",
        "        Processing Logic:\n",
        "            - Creates a matrix of size (m+1) x (n+1).\n",
        "            - Fills the first row and column with increasing numbers.\n",
        "            - Calculates the distance between the two strings using dynamic programming.\n",
        "            - Normalizes the distance by dividing it by the maximum length of the two strings.\n",
        "            - Returns the normalized distance.\"\"\"\n",
        "            \n",
        "        m, n = len(s1), len(s2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if s1[i - 1] == s2[j - 1]:\n",
        "                    dp[i][j] = dp[i - 1][j - 1]\n",
        "                else:\n",
        "                    dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
        "\n",
        "        distance = dp[m][n]\n",
        "        max_length = max(m, n)\n",
        "        normalized_distance = distance / max_length\n",
        "        return 1 - normalized_distance\n",
        "    \n",
        "    def _lcs_distance(self, s1: str, s2: str):\n",
        "        \"\"\"Docstring:\n",
        "        Calculates the longest common subsequence (LCS) distance between two strings.\n",
        "        Parameters:\n",
        "            - s1 (str): First string.\n",
        "            - s2 (str): Second string.\n",
        "        Returns:\n",
        "            - float: The LCS distance between the two strings.\n",
        "        Processing Logic:\n",
        "            - Uses dynamic programming.\n",
        "            - Calculates LCS length and max length.\n",
        "            - Normalizes distance and returns it.\"\"\"\n",
        "        m, n = len(s1), len(s2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if s1[i - 1] == s2[j - 1]:\n",
        "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "        lcs_length = dp[m][n]\n",
        "        max_length = max(m, n)\n",
        "        normalized_distance = 1 - (lcs_length / max_length)\n",
        "        return 1 - normalized_distance\n",
        "    \n",
        "    def _spell_check_at(self, sentence_list: list[str], candidate_ind: int, topk: int = 150):\n",
        "        \"\"\"Spell checks a word in a sentence by replacing it with a [MASK] token and predicting the most probable words using a pre-trained language model.\n",
        "        Parameters:\n",
        "            - sentence_list (list[str]): List of words in the sentence.\n",
        "            - candidate_ind (int): Index of the word to be spell checked.\n",
        "            - topk (int): Number of top predicted words to be returned. Default is 150.\n",
        "        Returns:\n",
        "            - output (dict): Dictionary containing the top predicted words and their corresponding scores.\n",
        "        Processing Logic:\n",
        "            - Replaces the word at candidate_ind with a [MASK] token.\n",
        "            - Encodes the sentence using a pre-trained tokenizer.\n",
        "            - Uses a pre-trained language model to predict the most probable words.\n",
        "            - Calculates a score for each predicted word based on its probability, Levenshtein distance, and longest common subsequence distance.\n",
        "            - Returns a dictionary of the top predicted words and their scores.\"\"\"\n",
        "        candidate = sentence_list[candidate_ind]\n",
        "        sentence_list[candidate_ind] = '[MASK]'\n",
        "        text = \" \".join(sentence_list)\n",
        "\n",
        "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids)\n",
        "            logits = output.logits\n",
        "\n",
        "        mask_logits = logits[0, mask_token_index, :]\n",
        "\n",
        "        probabilities = torch.softmax(mask_logits, dim=-1)\n",
        "\n",
        "        top_k_probs, top_k_tokens = torch.topk(probabilities, k=topk)\n",
        "        top_k_probs = top_k_probs[0]\n",
        "        top_k_tokens = top_k_tokens[0]\n",
        "        \n",
        "        top_k_words = [tokenizer.decode([token_id]) for token_id in top_k_tokens]\n",
        "\n",
        "        output = {}\n",
        "        for word, prob in zip(top_k_words, top_k_probs):\n",
        "            score = self._levenshtein_distance(word, candidate) + prob + self._lcs_distance(word, candidate)\n",
        "            output[word] = score\n",
        "        sentence_list[candidate_ind] = candidate\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Justify your decisions\n",
        "\n",
        "### Idea\n",
        "\n",
        "My idea is to check each word (let it be `w`) in the target sentence, get the probabilities of the top k words instead of `w` using a language model, calculate various metrics between `w` and each top k element and use a combination of probability and metrics as a word score instead of `w`.\n",
        "\n",
        "### Concrete implementation\n",
        "\n",
        "* using distilbert for language model - this model is simple enough that it can be run on a PC and works quite well for this task, since it is trained on a fairly good amount of data and can understand the context\n",
        "* using Levenshtein distance and longest common subsequence as metrics - I chose these metrics because I remember the ideas behind them and how to write them. I think there are many algorithms that can be used to compare strings, so it's hard to give a rationale for how to choose the best ones.\n",
        "* simple insertion of new metrics - you need to implement the function (should return a score from 0 to 1) and simply add its result\n",
        "\n",
        "### Possible updates\n",
        "\n",
        "* create weights for the measure - we can do this after collecting data sets with each feature score and determining whether the word with the highest score is the target word or not. Using the dataset, we can train a simple logistic regression to produce the weights.\n",
        "* add new algorithms - for example, phonetic algorithms or n-gram similarity\n",
        "* take a better language model or a model trained on text similar to the target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Mistaker:\n",
        "    def generate_spelling_mistakes(self, sentence: str, mistake_probability: float = 0.1):\n",
        "        \"\"\"Generates spelling mistakes in a sentence.\n",
        "        Parameters:\n",
        "            - sentence (str): The sentence to generate spelling mistakes in.\n",
        "            - mistake_probability (float): The probability of a word being misspelled, defaults to 0.1.\n",
        "        Returns:\n",
        "            - str: The sentence with spelling mistakes.\n",
        "        Processing Logic:\n",
        "            - Splits the sentence into words.\n",
        "            - Generates a list of misspelled words.\n",
        "            - Joins the misspelled words into a sentence.\"\"\"\n",
        "        words = sentence.split()\n",
        "        misspelled_words = []\n",
        "        \n",
        "        for word in words:\n",
        "            if random.random() < mistake_probability:\n",
        "                misspelled_word = self._introduce_mistake(word)\n",
        "                if misspelled_word:\n",
        "                    misspelled_words.append(misspelled_word) \n",
        "            else:\n",
        "                misspelled_words.append(word)\n",
        "        \n",
        "        return ' '.join(misspelled_words)\n",
        "\n",
        "    def _introduce_mistake(self, word: str):\n",
        "        mistake_type = random.randint(1, 4)\n",
        "        \n",
        "        if len(word) <= 2:\n",
        "            return None\n",
        "        \n",
        "        if mistake_type == 1:\n",
        "            # Delete a character\n",
        "            position = random.randint(0, len(word) - 1)\n",
        "            misspelled_word = word[:position] + word[position + 1:]\n",
        "        elif mistake_type == 2:\n",
        "            # Insert a character\n",
        "            position = random.randint(0, len(word))\n",
        "            inserted_char = random.choice(string.ascii_lowercase)\n",
        "            misspelled_word = word[:position] + inserted_char + word[position:]\n",
        "        elif mistake_type == 3:\n",
        "            # Substitute a character\n",
        "            position = random.randint(0, len(word) - 1)\n",
        "            substituted_char = random.choice(string.ascii_lowercase)\n",
        "            misspelled_word = word[:position] + substituted_char + word[position + 1:]\n",
        "        else:\n",
        "            # Transpose two adjacent characters\n",
        "            position = random.randint(0, len(word) - 2)\n",
        "            misspelled_word = word[:position] + word[position + 1] + word[position] + word[position + 2:]\n",
        "        \n",
        "        return misspelled_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Norvig Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestSolutions:\n",
        "    def __init__(self, dataset_path='singapore_airlines_reviews.csv', device='cpu'):\n",
        "        self.spell_checker = SpellChecker(device)\n",
        "        self.test_data = pd.read_csv(dataset_path).title.values[:500]\n",
        "        self.mistaker = Mistaker()\n",
        "        \n",
        "    def run(self):\n",
        "        test_set = [self.mistaker.generate_spelling_mistakes(text) for text in self.test_data]\n",
        "        \n",
        "        start_norvig = time.time()\n",
        "        scores_norvig = [self._eval_norvig_sentence(self.test_data[i], test_set[i]) for i in range(len(test_set)) if test_set[i]]\n",
        "        end_norvig = time.time()\n",
        "        \n",
        "        start_spell_checker = time.time()\n",
        "        scores_spell_checker = [self._eval_my_checker_sentence(self.test_data[i], test_set[i]) for i in range(len(test_set)) if test_set[i]]\n",
        "        end_spell_checker = time.time()\n",
        "        \n",
        "        print('Norvig exucution time: ', end_norvig - start_norvig)\n",
        "        print('My spell checker exucution time: ', end_spell_checker - start_spell_checker)\n",
        "        \n",
        "        return sum(scores_norvig) / len(scores_norvig), sum(scores_spell_checker) / len(scores_spell_checker)\n",
        "        \n",
        "        \n",
        "    def _eval_norvig_sentence(self, correct_sentence, incorrect_sentence):\n",
        "        total_words = 0\n",
        "        correct_words = 0\n",
        "        \n",
        "        for correct, incorrect in zip(correct_sentence.split(), incorrect_sentence.split()):\n",
        "            total_words += 1\n",
        "            corrected_word = correction(incorrect)\n",
        "            \n",
        "            if correct.lower() == corrected_word.lower():\n",
        "                correct_words += 1\n",
        "        \n",
        "        return correct_words / total_words\n",
        "    \n",
        "    def _eval_my_checker_sentence(self, correct_sentence, incorrect_sentence):\n",
        "        total_words = 0\n",
        "        correct_words = 0\n",
        "        corrected = self.spell_checker.spell_check(incorrect_sentence)\n",
        "        for correct, corrected_word in zip(correct_sentence.split(), corrected.split()):\n",
        "            total_words += 1\n",
        "            \n",
        "            if correct.lower() == corrected_word.lower():\n",
        "                correct_words += 1\n",
        "        \n",
        "        return correct_words / total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig exucution time:  34.195584297180176\n",
            "My spell checker exucution time:  91.62865161895752\n"
          ]
        }
      ],
      "source": [
        "tester = TestSolutions(device='cuda')\n",
        "norvig_score, spell_checker_score = tester.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norvig accuracy:  0.7464436674522592\n",
            "My spell checker accuracy:  0.7781652153488069\n"
          ]
        }
      ],
      "source": [
        "print(\"Norvig accuracy: \", norvig_score)\n",
        "print('My spell checker accuracy: ', spell_checker_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
