{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvigâ€™s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/slewie/anaconda3/envs/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import torch\n",
        "from collections import Counter\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"distilbert-base-uncased\", device_map='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SpellChecker:\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "    \n",
        "    def spell_check(self, sentence):\n",
        "        sentence_list = sentence.split()\n",
        "        res = [self._spell_check_at(sentence_list, i) if sentence_list[i].lower() not in WORDS else sentence_list[i] for i in range(len(sentence_list))]\n",
        "    \n",
        "        corrected_sentence = []\n",
        "        for my_dict in res:\n",
        "            if isinstance(my_dict, str):\n",
        "                corrected_sentence.append(my_dict)\n",
        "                continue\n",
        "            top_keys = sorted(my_dict, key=my_dict.get, reverse=True)[:1][0]\n",
        "            corrected_sentence.append(top_keys)\n",
        "            \n",
        "        return \" \".join(corrected_sentence)\n",
        "    \n",
        "    def _levenshtein_distance(self, s1, s2):\n",
        "        m, n = len(s1), len(s2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if s1[i - 1] == s2[j - 1]:\n",
        "                    dp[i][j] = dp[i - 1][j - 1]\n",
        "                else:\n",
        "                    dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
        "\n",
        "        distance = dp[m][n]\n",
        "        max_length = max(m, n)\n",
        "        normalized_distance = distance / max_length\n",
        "        return 1 - normalized_distance\n",
        "    \n",
        "    def _lcs_distance(self, s1, s2):\n",
        "        m, n = len(s1), len(s2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if s1[i - 1] == s2[j - 1]:\n",
        "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "        lcs_length = dp[m][n]\n",
        "        max_length = max(m, n)\n",
        "        normalized_distance = 1 - (lcs_length / max_length)\n",
        "        return 1 - normalized_distance\n",
        "    \n",
        "    def _spell_check_at(self, sentence_list, candidate_ind, topk=150):\n",
        "        candidate = sentence_list[candidate_ind]\n",
        "        sentence_list[candidate_ind] = '[MASK]'\n",
        "        text = \" \".join(sentence_list)\n",
        "\n",
        "        input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(input_ids)\n",
        "            logits = output.logits\n",
        "\n",
        "        mask_logits = logits[0, mask_token_index, :]\n",
        "\n",
        "        probabilities = torch.softmax(mask_logits, dim=-1)\n",
        "\n",
        "        top_k_probs, top_k_tokens = torch.topk(probabilities, k=topk)\n",
        "        top_k_probs = top_k_probs[0]\n",
        "        top_k_tokens = top_k_tokens[0]\n",
        "        \n",
        "        top_k_words = [tokenizer.decode([token_id]) for token_id in top_k_tokens]\n",
        "\n",
        "        output = {}\n",
        "        for word, prob in zip(top_k_words, top_k_probs):\n",
        "            score = self._levenshtein_distance(word, candidate) + prob + self._lcs_distance(word, candidate)\n",
        "            output[word] = score\n",
        "        sentence_list[candidate_ind] = candidate\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "spell_checker = SpellChecker('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I love my cat and my dog'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spell_checker.spell_check('I love my kat and my dogg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Justify your decisions\n",
        "\n",
        "### Language Model\n",
        "I used the DistilBERT language model from the Transformers library for context-aware spelling correction. DistilBERT is a smaller and faster version of BERT, which is a powerful transformer-based language model capable of capturing contextual information. Using a pre-trained language model like DistilBERT allows leveraging the learned language representations to make more informed decisions about spelling corrections based on the surrounding context.\n",
        "\n",
        "### Spelling Correction Algorithm\n",
        "The implemented spelling correction algorithm is inspired by Norvig's approach but incorporates the language model for context-aware corrections. The core idea is to generate a list of candidate corrections for a potentially misspelled word and then score these candidates based on their conditional probabilities obtained from the language model and their edit distance from the original word.\n",
        "\n",
        "For each potentially misspelled word, the algorithm masks it in the input sentence and uses the language model to generate the top-k most probable token predictions at that position. These top-k tokens are then scored by combining their language model probabilities and their edit distances from the original word. The word with the highest combined score is chosen as the correction.\n",
        "\n",
        "### Scoring Function\n",
        "The scoring function used to rank the candidate corrections is a simple linear combination of the language model probability and the edit distance ratio. The language model probability captures the contextual fit of the candidate word, while the edit distance ratio measures the similarity to the original word. By combining these two factors, the algorithm can balance contextual relevance and similarity to the misspelled word.\n",
        "\n",
        "### Hyperparameters\n",
        "The implementation includes a `topk` hyperparameter that determines the number of top candidate corrections to consider from the language model. A larger value of `topk` increases the diversity of candidates but may also introduce more irrelevant options. In this implementation, `topk` is set to 150, which should provide a reasonable balance between diversity and computational efficiency.\n",
        "\n",
        "### Efficiency Considerations\n",
        "To improve efficiency, the implementation leverages PyTorch's GPU acceleration capabilities by running the language model on a GPU (if available). This can significantly speed up the inference process, especially for longer input sequences.\n",
        "\n",
        "### Potential Improvements\n",
        "While the current implementation provides a decent starting point, there are several areas for potential improvement:\n",
        "\n",
        "1. **Weighted Scoring**: Instead of a simple linear combination, a more sophisticated weighting scheme could be explored to better balance the language model probabilities and edit distances.\n",
        "2. **Beam Search**: Incorporating beam search or other decoding strategies could potentially improve the quality of corrections by considering multiple candidates simultaneously and allowing for better exploration of the search space.\n",
        "3. **Language-Specific Considerations**: The current implementation is focused on English text. For other languages, additional language-specific considerations (e.g., character sets, common misspellings, morphological rules) may need to be incorporated.\n",
        "4. **Vocabulary Expansion**: The current approach is limited to the vocabulary of the pre-trained language model. Techniques like character-level modeling or vocabulary expansion could help handle out-of-vocabulary words more effectively.\n",
        "5. **Contextual Edit Distance**: The current implementation uses a simple edit distance metric. Incorporating contextual information into the edit distance calculation (e.g., considering common misspelling patterns or keyboard layout) could further improve the spelling correction accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Mistaker:\n",
        "    def generate_spelling_mistakes(self, sentence, mistake_probability=0.1):\n",
        "        words = sentence.split()\n",
        "        misspelled_words = []\n",
        "        \n",
        "        for word in words:\n",
        "            if random.random() < mistake_probability:\n",
        "                misspelled_word = self._introduce_mistake(word)\n",
        "                if misspelled_word:\n",
        "                    misspelled_words.append(misspelled_word) \n",
        "            else:\n",
        "                misspelled_words.append(word)\n",
        "        \n",
        "        return ' '.join(misspelled_words)\n",
        "\n",
        "    def _introduce_mistake(self, word):\n",
        "        mistake_type = random.randint(1, 4)\n",
        "        \n",
        "        if len(word) <= 2:\n",
        "            return None\n",
        "        \n",
        "        if mistake_type == 1:\n",
        "            # Delete a character\n",
        "            position = random.randint(0, len(word) - 1)\n",
        "            misspelled_word = word[:position] + word[position + 1:]\n",
        "        elif mistake_type == 2:\n",
        "            # Insert a character\n",
        "            position = random.randint(0, len(word))\n",
        "            inserted_char = random.choice(string.ascii_lowercase)\n",
        "            misspelled_word = word[:position] + inserted_char + word[position:]\n",
        "        elif mistake_type == 3:\n",
        "            # Substitute a character\n",
        "            position = random.randint(0, len(word) - 1)\n",
        "            substituted_char = random.choice(string.ascii_lowercase)\n",
        "            misspelled_word = word[:position] + substituted_char + word[position + 1:]\n",
        "        else:\n",
        "            # Transpose two adjacent characters\n",
        "            position = random.randint(0, len(word) - 2)\n",
        "            misspelled_word = word[:position] + word[position + 1] + word[position] + word[position + 2:]\n",
        "        \n",
        "        return misspelled_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestSolutions:\n",
        "    def __init__(self, dataset_path='singapore_airlines_reviews.csv'):\n",
        "        self.spell_checker = SpellChecker('cuda')\n",
        "        self.test_data = pd.read_csv(dataset_path).title.values[:50]\n",
        "        self.mistaker = Mistaker()\n",
        "        \n",
        "    def run(self):\n",
        "        test_set = [self.mistaker.generate_spelling_mistakes(text) for text in self.test_data]\n",
        "        \n",
        "        scores_norvig = [self._eval_norvig_sentence(self.test_data[i], test_set[i]) for i in range(len(test_set)) if test_set[i]]\n",
        "        scores_spell_checker = [self._eval_my_checker_sentence(self.test_data[i], test_set[i]) for i in range(len(test_set)) if test_set[i]]\n",
        "        \n",
        "        return sum(scores_norvig) / len(scores_norvig), sum(scores_spell_checker) / len(scores_spell_checker), scores_norvig, scores_spell_checker, test_set\n",
        "        \n",
        "        \n",
        "    def _eval_norvig_sentence(self, correct_sentence, incorrect_sentence):\n",
        "        total_words = 0\n",
        "        correct_words = 0\n",
        "        \n",
        "        for correct, incorrect in zip(correct_sentence.split(), incorrect_sentence.split()):\n",
        "            total_words += 1\n",
        "            corrected_word = correction(incorrect)\n",
        "            \n",
        "            if correct.lower() == corrected_word.lower():\n",
        "                correct_words += 1\n",
        "        \n",
        "        return correct_words / total_words\n",
        "    \n",
        "    def _eval_my_checker_sentence(self, correct_sentence, incorrect_sentence):\n",
        "        total_words = 0\n",
        "        correct_words = 0\n",
        "        corrected = spell_checker.spell_check(incorrect_sentence)\n",
        "        for correct, corrected_word in zip(correct_sentence.split(), corrected.split()):\n",
        "            total_words += 1\n",
        "            \n",
        "            if correct.lower() == corrected_word.lower():\n",
        "                correct_words += 1\n",
        "        \n",
        "        return correct_words / total_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7365858585858586, 0.7868715728715729)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = TestSolutions()\n",
        "a, b, c, d, e = t.run()\n",
        "a, b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
